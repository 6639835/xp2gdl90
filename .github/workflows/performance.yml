name: Performance Monitoring

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run performance tests weekly on Sundays at 2 AM UTC
    - cron: '0 2 * * 0'
  workflow_dispatch:
    inputs:
      benchmark_filter:
        description: 'Benchmark filter pattern (regex)'
        required: false
        default: '.*'
      compare_baseline:
        description: 'Compare against baseline'
        type: boolean
        default: true

permissions:
  contents: write
  actions: read
  pull-requests: write

jobs:
  benchmark:
    name: Performance Benchmarks
    runs-on: ${{ matrix.os }}
    
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        include:
          - os: ubuntu-latest
            platform: linux
          - os: macos-latest
            platform: mac
          - os: windows-latest
            platform: windows
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.x'
    
    - name: Install dependencies (Linux)
      if: runner.os == 'Linux'
      run: |
        sudo apt-get update
        sudo apt-get install -y cmake build-essential libbenchmark-dev
    
    - name: Install dependencies (macOS)
      if: runner.os == 'macOS'
      run: |
        # Handle cmake conflicts gracefully
        if brew list cmake &>/dev/null; then
          echo "CMake already installed, skipping..."
        else
          brew install cmake
        fi
        
        # Check system and compiler architecture
        SYSTEM_ARCH=$(uname -m)
        COMPILER_ARCH=$(clang -dumpmachine 2>/dev/null | cut -d'-' -f1 || echo "unknown")
        echo "System architecture: $SYSTEM_ARCH"
        echo "Compiler target: $COMPILER_ARCH"
        
        # GitHub Actions macOS runners are x86_64, but Homebrew on Apple Silicon defaults to arm64
        # This creates architecture mismatches that we need to detect and handle
        
        echo "Attempting to install Google Benchmark..."
        if brew install google-benchmark; then
          echo "✅ Google Benchmark installed"
          
          # Check for architecture compatibility issues
          BENCHMARK_COMPATIBLE=true
          
          if [ -f "/opt/homebrew/lib/libbenchmark.a" ]; then
            BENCHMARK_ARCHS=$(lipo -archs /opt/homebrew/lib/libbenchmark.a 2>/dev/null || echo "unknown")
            echo "Benchmark library architectures: $BENCHMARK_ARCHS"
            
            # Check if our target architecture is supported by the benchmark library
            if [[ "$BENCHMARK_ARCHS" != *"$SYSTEM_ARCH"* ]] && [[ "$BENCHMARK_ARCHS" != "unknown" ]]; then
              echo "⚠️  Architecture mismatch detected:"
              echo "   System: $SYSTEM_ARCH"
              echo "   Benchmark library: $BENCHMARK_ARCHS"
              echo "   This will cause linking failures"
              BENCHMARK_COMPATIBLE=false
            fi
          elif [ -f "/usr/local/lib/libbenchmark.a" ]; then
            BENCHMARK_ARCHS=$(lipo -archs /usr/local/lib/libbenchmark.a 2>/dev/null || echo "unknown")
            echo "Benchmark library architectures (Intel): $BENCHMARK_ARCHS"
          else
            echo "⚠️  Could not find benchmark library files"
            BENCHMARK_COMPATIBLE=false
          fi
          
          if [ "$BENCHMARK_COMPATIBLE" = false ]; then
            echo "🚫 Disabling benchmarks due to architecture incompatibility"
            echo "BENCHMARK_AVAILABLE=false" >> $GITHUB_ENV
            echo "BENCHMARK_SKIP_REASON=architecture_mismatch" >> $GITHUB_ENV
          else
            echo "✅ Benchmark architecture compatibility verified"
            echo "BENCHMARK_AVAILABLE=true" >> $GITHUB_ENV
          fi
        else
          echo "❌ Failed to install Google Benchmark"
          echo "BENCHMARK_AVAILABLE=false" >> $GITHUB_ENV
          echo "BENCHMARK_SKIP_REASON=install_failed" >> $GITHUB_ENV
        fi
    
    - name: Install dependencies (Windows)
      if: runner.os == 'Windows'
      run: |
        choco install cmake
        vcpkg install benchmark:x64-windows
      shell: pwsh
    
    - name: Configure CMake
      run: |
        mkdir build
        cd build
        
        # Determine benchmark availability
        ENABLE_BENCHMARKS=ON
        BUILD_REASON="enabled"
        
        if [ "${{ env.BENCHMARK_AVAILABLE }}" == "false" ]; then
          ENABLE_BENCHMARKS=OFF
          BUILD_REASON="${{ env.BENCHMARK_SKIP_REASON }}"
          
          case "$BUILD_REASON" in
            "architecture_mismatch")
              echo "🚫 Benchmarks disabled: Architecture mismatch between system and benchmark library"
              ;;
            "install_failed")
              echo "🚫 Benchmarks disabled: Failed to install Google Benchmark library"
              ;;
            *)
              echo "🚫 Benchmarks disabled: Dependency issues detected"
              ;;
          esac
        else
          echo "✅ Benchmarks enabled: All dependencies available and compatible"
        fi
        
        # Configure with platform-specific settings
        CMAKE_ARGS="-DCMAKE_BUILD_TYPE=Release -DBUILD_BENCHMARKS=$ENABLE_BENCHMARKS"
        
        if [[ "$RUNNER_OS" == "macOS" ]]; then
          # Set architecture explicitly for macOS
          ARCH=$(uname -m)
          CMAKE_ARGS="$CMAKE_ARGS -DCMAKE_OSX_ARCHITECTURES=$ARCH"
          
          echo "Configuring for macOS ($ARCH architecture):"
          echo "  - Build type: Release"
          echo "  - Benchmarks: $ENABLE_BENCHMARKS"
          echo "  - Target architecture: $ARCH"
          
          # Add additional macOS-specific flags if benchmarks are disabled due to architecture
          if [ "$BUILD_REASON" == "architecture_mismatch" ]; then
            # Force disable any benchmark-related CMake searching
            CMAKE_ARGS="$CMAKE_ARGS -Dbenchmark_ROOT=NOTFOUND -DCMAKE_DISABLE_FIND_PACKAGE_benchmark=TRUE"
            echo "  - Benchmark package search: disabled"
          fi
        else
          echo "Configuring for $RUNNER_OS:"
          echo "  - Build type: Release" 
          echo "  - Benchmarks: $ENABLE_BENCHMARKS"
        fi
        
        # Run CMake configuration
        echo "Running: cmake $CMAKE_ARGS .."
        if cmake $CMAKE_ARGS ..; then
          echo "✅ CMake configuration successful"
        else
          echo "❌ CMake configuration failed"
          exit 1
        fi
    
    - name: Build benchmarks
      run: |
        cd build
        
        # Check if benchmarks were supposed to be built
        if [ "${{ env.BENCHMARK_AVAILABLE }}" == "false" ]; then
          echo "⏩ Skipping benchmark build (benchmarks disabled: ${{ env.BENCHMARK_SKIP_REASON }})"
          echo "BENCHMARKS_BUILT=false" >> $GITHUB_ENV
          
          # Build main project only
          echo "Building main plugin only..."
          if cmake --build . --config Release --target xp2gdl90; then
            echo "✅ Main plugin build completed successfully"
            
            # Check that the main plugin was built
            if [ -f "mac.xpl" ] || [ -f "lin.xpl" ] || [ -f "win.xpl" ]; then
              echo "✅ Plugin executable found"
            else
              echo "⚠️  Plugin executable not found after build"
              ls -la *.xpl || echo "No .xpl files found"
            fi
          else
            echo "❌ Main plugin build failed"
            exit 1
          fi
          
          exit 0  # Skip to next step
        fi
        
        # Build everything including benchmarks
        echo "Building project with benchmarks..."
        if cmake --build . --config Release; then
          echo "✅ Build completed successfully"
          
          # Check if benchmarks were actually built
          BENCHMARK_PATHS=(
            "benchmarks/xp2gdl90_benchmarks"
            "xp2gdl90_benchmarks" 
            "benchmarks/Release/xp2gdl90_benchmarks.exe"
            "Release/xp2gdl90_benchmarks.exe"
          )
          
          BENCHMARK_FOUND=false
          for bench_path in "${BENCHMARK_PATHS[@]}"; do
            if [ -f "$bench_path" ]; then
              echo "✅ Benchmark executable found: $bench_path"
              BENCHMARK_FOUND=true
              break
            fi
          done
          
          if [ "$BENCHMARK_FOUND" = true ]; then
            echo "BENCHMARKS_BUILT=true" >> $GITHUB_ENV
          else
            echo "⚠️  Build succeeded but benchmark executable not found"
            echo "This indicates a configuration issue even though benchmarks were enabled"
            echo "BENCHMARKS_BUILT=false" >> $GITHUB_ENV
            
            # Debugging information
            echo ""
            echo "Searched for benchmark executable in:"
            printf '%s\n' "${BENCHMARK_PATHS[@]}"
            echo ""
            echo "Files in build directory:"
            find . -name "*benchmark*" -type f 2>/dev/null || echo "No benchmark files found"
            echo ""
            echo "Available executables:"
            find . -name "*.exe" -o -name "*benchmark*" -type f 2>/dev/null || echo "No executables found"
          fi
        else
          echo "❌ Build failed"
          echo "BENCHMARKS_BUILT=false" >> $GITHUB_ENV
          
          # Try to provide some diagnostic information
          echo ""
          echo "Build failure diagnostic information:"
          echo "- OS: ${{ runner.os }}"
          echo "- Architecture: $(uname -m)"
          echo "- Benchmark availability: ${{ env.BENCHMARK_AVAILABLE }}"
          if [ "${{ env.BENCHMARK_AVAILABLE }}" == "false" ]; then
            echo "- Skip reason: ${{ env.BENCHMARK_SKIP_REASON }}"
          fi
          
          exit 1
        fi
    
    - name: Run benchmarks
      id: benchmark
      run: |
        cd build
        
        # Check if benchmarks were built successfully
        if [ "${{ env.BENCHMARKS_BUILT }}" != "true" ]; then
          echo "⚠️  Benchmarks were not built successfully, creating placeholder results"
          
          # Create placeholder JSON result
          cat > benchmark_results_${{ matrix.platform }}.json << 'EOF'
        {
          "context": {
            "date": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "host_name": "${{ runner.os }}",
            "executable": "xp2gdl90_benchmarks",
            "num_cpus": 1,
            "mhz_per_cpu": 2000,
            "cpu_scaling_enabled": false,
            "caches": []
          },
          "benchmarks": [
            {
              "name": "BenchmarkSkipped",
              "family_index": 0,
              "per_family_instance_index": 0,
              "run_name": "BenchmarkSkipped",
              "run_type": "iteration",
              "repetitions": 1,
              "repetition_index": 0,
              "threads": 1,
              "iterations": 1,
              "cpu_time": 0,
              "real_time": 0,
              "time_unit": "ns"
            }
          ]
        }
        EOF
          
          # Create placeholder text result
          echo "Benchmarks were skipped due to build issues on ${{ matrix.platform }}" > benchmark_results_${{ matrix.platform }}.txt
          echo "This may be due to:" >> benchmark_results_${{ matrix.platform }}.txt
          echo "- Architecture mismatch (arm64 vs x86_64)" >> benchmark_results_${{ matrix.platform }}.txt
          echo "- Missing Google Benchmark library" >> benchmark_results_${{ matrix.platform }}.txt
          echo "- Build configuration issues" >> benchmark_results_${{ matrix.platform }}.txt
          echo ""
          echo "The main plugin build was successful." >> benchmark_results_${{ matrix.platform }}.txt
          
          echo "BENCHMARK_STATUS=skipped" >> $GITHUB_ENV
          exit 0
        fi
        
        # Set benchmark filter
        FILTER="${{ github.event.inputs.benchmark_filter || '.*' }}"
        
        # Find the benchmark executable
        BENCHMARK_EXE=""
        for possible_exe in "./benchmarks/xp2gdl90_benchmarks" "./xp2gdl90_benchmarks" "./benchmarks/Release/xp2gdl90_benchmarks.exe"; do
          if [ -f "$possible_exe" ]; then
            BENCHMARK_EXE="$possible_exe"
            break
          fi
        done
        
        if [ -z "$BENCHMARK_EXE" ]; then
          echo "Error: Benchmark executable not found after successful build"
          echo "Searched locations:"
          echo "- ./benchmarks/xp2gdl90_benchmarks"
          echo "- ./xp2gdl90_benchmarks"
          echo "- ./benchmarks/Release/xp2gdl90_benchmarks.exe"
          echo ""
          echo "Available files:"
          find . -name "*benchmark*" -type f
          echo "BENCHMARK_STATUS=error" >> $GITHUB_ENV
          exit 1
        fi
        
        echo "✅ Found benchmark executable: $BENCHMARK_EXE"
        
        # Run benchmarks with JSON output
        if $BENCHMARK_EXE \
          --benchmark_filter="$FILTER" \
          --benchmark_format=json \
          --benchmark_out=benchmark_results_${{ matrix.platform }}.json \
          --benchmark_repetitions=3 \
          --benchmark_report_aggregates_only=true; then
          
          # Also create human-readable output
          $BENCHMARK_EXE \
            --benchmark_filter="$FILTER" \
            --benchmark_repetitions=3 \
            --benchmark_report_aggregates_only=true \
            > benchmark_results_${{ matrix.platform }}.txt
          
          echo "✅ Benchmarks completed successfully"
          echo "BENCHMARK_STATUS=success" >> $GITHUB_ENV
        else
          echo "❌ Benchmark execution failed"
          echo "BENCHMARK_STATUS=failed" >> $GITHUB_ENV
          exit 1
        fi
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ matrix.platform }}
        path: |
          build/benchmark_results_${{ matrix.platform }}.json
          build/benchmark_results_${{ matrix.platform }}.txt
        retention-days: 30
    
    - name: Benchmark Summary
      run: |
        cd build
        
        # Determine overall status
        OVERALL_STATUS="${{ env.BENCHMARK_STATUS }}"
        
        # If no benchmark status was set, determine from build status and availability
        if [ -z "$OVERALL_STATUS" ] || [ "$OVERALL_STATUS" = "null" ]; then
          if [ "${{ env.BENCHMARK_AVAILABLE }}" == "false" ]; then
            OVERALL_STATUS="skipped"
          elif [ "${{ env.BENCHMARKS_BUILT }}" == "false" ]; then
            OVERALL_STATUS="build_failed"  
          else
            OVERALL_STATUS="unknown"
          fi
        fi
        
        echo "📊 **Benchmark Summary for ${{ matrix.platform }}**"
        echo ""
        
        case "$OVERALL_STATUS" in
          "success")
            echo "✅ **Status: Completed Successfully**"
            echo ""
            if [ -f "benchmark_results_${{ matrix.platform }}.txt" ]; then
              echo "**Sample Results:**"
              echo "\`\`\`"
              head -n 15 "benchmark_results_${{ matrix.platform }}.txt" || echo "Could not read results"
              echo "\`\`\`"
            fi
            echo ""
            echo "📈 Full results available in workflow artifacts"
            ;;
            
          "skipped")
            echo "⏩ **Status: Skipped (Expected)**"
            echo ""
            
            SKIP_REASON="${{ env.BENCHMARK_SKIP_REASON }}"
            case "$SKIP_REASON" in
              "architecture_mismatch")
                echo "**Reason:** Architecture Compatibility Issue"
                echo ""
                echo "- 🏗️  **System**: GitHub Actions macOS runner (x86_64)"
                echo "- 📚 **Library**: Google Benchmark installed via Homebrew (arm64)"  
                echo "- ⚖️  **Result**: Architecture mismatch prevents linking"
                echo ""
                echo "**This is normal and expected behavior in CI environments.**"
                echo "The workflow automatically detects this and gracefully skips benchmarks."
                ;;
              "install_failed")
                echo "**Reason:** Dependency Installation Failed"
                echo ""
                echo "Google Benchmark could not be installed on this platform."
                ;;
              *)
                echo "**Reason:** Dependency Issues"
                echo ""
                echo "Benchmarks were disabled due to detected compatibility problems."
                ;;
            esac
            
            echo ""
            echo "✅ **Main plugin build**: Completed successfully"
            echo "📄 **Analysis data**: Placeholder results generated for consistency"
            echo "🔧 **Local development**: Benchmarks will work on matching architectures"
            ;;
            
          "build_failed")
            echo "❌ **Status: Build Failed**"
            echo ""
            echo "The benchmark build failed even though dependencies appeared available."
            echo "This suggests a configuration or compatibility issue."
            echo ""
            echo "**Diagnostic Info:**"
            echo "- Benchmark availability: ${{ env.BENCHMARK_AVAILABLE }}"
            echo "- Skip reason: ${{ env.BENCHMARK_SKIP_REASON }}"
            echo "- OS: ${{ runner.os }}"
            echo "- Architecture: $(uname -m 2>/dev/null || echo 'unknown')"
            ;;
            
          "failed")
            echo "❌ **Status: Execution Failed**"
            echo ""
            echo "Benchmarks were built successfully but failed to execute."
            echo "Check the build logs above for specific error details."
            ;;
            
          "error"|"unknown"|*)
            echo "❓ **Status: Unknown Error**"
            echo ""
            echo "An unexpected issue occurred during benchmark processing."
            echo ""
            echo "**Debug Info:**"
            echo "- Benchmark status: $OVERALL_STATUS"
            echo "- Available: ${{ env.BENCHMARK_AVAILABLE }}"
            echo "- Built: ${{ env.BENCHMARKS_BUILT }}"
            echo "- Skip reason: ${{ env.BENCHMARK_SKIP_REASON }}"
            ;;
        esac
    
    - name: Store benchmark results
      if: github.ref == 'refs/heads/main' && env.BENCHMARK_STATUS == 'success'
      uses: benchmark-action/github-action-benchmark@v1
      continue-on-error: true
      with:
        name: XP2GDL90 Benchmarks (${{ matrix.platform }})
        tool: 'googlecpp'
        output-file-path: build/benchmark_results_${{ matrix.platform }}.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
        # Store results in gh-pages branch
        gh-pages-branch: gh-pages
        benchmark-data-dir-path: benchmarks/${{ matrix.platform }}
        # Comment on PRs with performance comparison
        comment-on-alert: true
        alert-threshold: '150%'
        comment-always: ${{ github.event_name == 'pull_request' }}
        summary-always: true

  performance-analysis:
    name: Performance Analysis
    needs: benchmark
    runs-on: ubuntu-latest
    if: always()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.x'
    
    - name: Install analysis tools
      run: |
        pip install pandas matplotlib seaborn numpy jinja2
    
    - name: Download all benchmark results
      uses: actions/download-artifact@v4
      with:
        path: benchmark-artifacts/
      continue-on-error: true
    
    - name: Check for benchmark artifacts
      run: |
        if [ ! -d "benchmark-artifacts" ]; then
          echo "Creating benchmark-artifacts directory (no artifacts found)"
          mkdir -p benchmark-artifacts
        fi
        echo "Benchmark artifacts directory contents:"
        ls -la benchmark-artifacts/ || echo "No artifacts available"
    
    - name: Analyze performance results
      run: |
        python << 'EOF'
        import json
        import pandas as pd
        import matplotlib.pyplot as plt
        import numpy as np
        from pathlib import Path
        import os
        
        # Load benchmark results from all platforms
        results = {}
        artifact_dir = Path("benchmark-artifacts")
        
        # Check if the directory exists and has contents
        if not artifact_dir.exists():
            print("No benchmark artifacts directory found")
            artifact_dir.mkdir(exist_ok=True)
        
        if artifact_dir.exists():
            try:
                for platform_dir in artifact_dir.iterdir():
                    if platform_dir.is_dir():
                        platform = platform_dir.name.replace("benchmark-results-", "")
                        json_file = platform_dir / f"benchmark_results_{platform}.json"
                        
                        if json_file.exists():
                            with open(json_file) as f:
                                data = json.load(f)
                                results[platform] = data
                        else:
                            print(f"No JSON results found for platform: {platform}")
                    else:
                        print(f"Found non-directory item: {platform_dir}")
            except Exception as e:
                print(f"Error processing artifacts: {e}")
                results = {}
        else:
            print("Benchmark artifacts directory does not exist")
        
        # Create performance summary
        summary = []
        
        for platform, data in results.items():
            if 'benchmarks' in data:
                for bench in data['benchmarks']:
                    summary.append({
                        'platform': platform,
                        'benchmark': bench['name'],
                        'cpu_time': bench.get('cpu_time', 0),
                        'real_time': bench.get('real_time', 0),
                        'iterations': bench.get('iterations', 0),
                        'bytes_per_second': bench.get('bytes_per_second', 0),
                        'items_per_second': bench.get('items_per_second', 0)
                    })
        
        # Convert to DataFrame
        df = pd.DataFrame(summary)
        
        # Generate summary report
        with open('performance_summary.md', 'w') as f:
            f.write("# Performance Analysis Report\n\n")
            f.write(f"**Generated**: {pd.Timestamp.now()}\n")
            f.write(f"**Commit**: {os.environ.get('GITHUB_SHA', 'unknown')[:8]}\n\n")
            
            if not df.empty:
                f.write("## Performance Summary by Platform\n\n")
                
                # Group by platform and show averages
                platform_summary = df.groupby('platform').agg({
                    'cpu_time': 'mean',
                    'real_time': 'mean',
                    'iterations': 'sum'
                }).round(2)
                
                f.write(platform_summary.to_markdown())
                f.write("\n\n")
                
                # Top 10 fastest operations
                f.write("## Top 10 Fastest Operations\n\n")
                fastest = df.nsmallest(10, 'cpu_time')[['platform', 'benchmark', 'cpu_time']]
                f.write(fastest.to_markdown(index=False))
                f.write("\n\n")
                
                # Performance concerns (operations > 1ms)
                slow_ops = df[df['cpu_time'] > 1000000]  # > 1ms in nanoseconds
                if not slow_ops.empty:
                    f.write("## Performance Concerns (>1ms)\n\n")
                    f.write(slow_ops[['platform', 'benchmark', 'cpu_time']].to_markdown(index=False))
                    f.write("\n\n")
            else:
                f.write("No benchmark data available.\n")
        
        print("Performance analysis complete")
        EOF
    
    - name: Create performance charts
      run: |
        python << 'EOF'
        import json
        import matplotlib.pyplot as plt
        import numpy as np
        from pathlib import Path
        
        # Load results again for plotting
        results = {}
        artifact_dir = Path("benchmark-artifacts")
        
        # Check if the directory exists and has contents
        if artifact_dir.exists():
            try:
                for platform_dir in artifact_dir.iterdir():
                    if platform_dir.is_dir():
                        platform = platform_dir.name.replace("benchmark-results-", "")
                        json_file = platform_dir / f"benchmark_results_{platform}.json"
                        
                        if json_file.exists():
                            with open(json_file) as f:
                                data = json.load(f)
                                results[platform] = data
            except Exception as e:
                print(f"Error loading benchmark data for charts: {e}")
                results = {}
        else:
            print("No benchmark artifacts directory found for charts")
        
        if results:
            # Create performance comparison chart
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            # Chart 1: Average CPU time by platform
            platforms = []
            avg_times = []
            
            for platform, data in results.items():
                if 'benchmarks' in data:
                    times = [b.get('cpu_time', 0) for b in data['benchmarks']]
                    if times:
                        platforms.append(platform)
                        avg_times.append(np.mean(times) / 1000)  # Convert to microseconds
            
            if platforms:
                ax1.bar(platforms, avg_times)
                ax1.set_title('Average CPU Time by Platform')
                ax1.set_ylabel('Time (microseconds)')
                ax1.tick_params(axis='x', rotation=45)
            
            # Chart 2: Operation distribution
            all_times = []
            labels = []
            
            for platform, data in results.items():
                if 'benchmarks' in data:
                    for bench in data['benchmarks']:
                        # Group similar benchmarks
                        bench_name = bench['name'].split('/')[0]  # Remove parameters
                        all_times.append(bench.get('cpu_time', 0) / 1000)
                        labels.append(bench_name)
            
            if all_times:
                # Show distribution of operation times
                ax2.hist(all_times, bins=20, alpha=0.7)
                ax2.set_title('Distribution of Operation Times')
                ax2.set_xlabel('Time (microseconds)')
                ax2.set_ylabel('Count')
                ax2.set_yscale('log')
            
            plt.tight_layout()
            plt.savefig('performance_charts.png', dpi=150, bbox_inches='tight')
            print("Performance charts created")
        else:
            print("No data available for charts")
        EOF
    
    - name: Update performance summary
      run: |
        cat >> $GITHUB_STEP_SUMMARY << 'EOF'
        # 📊 Performance Analysis Results
        
        ## Benchmark Status
        EOF
        
        # Add status for each platform
        if [ -d "benchmark-artifacts" ]; then
          cd benchmark-artifacts
          for dir in benchmark-results-*; do
            if [ -d "$dir" ]; then
              platform=${dir#benchmark-results-}
              if [ -f "$dir/benchmark_results_${platform}.json" ]; then
                echo "✅ **$platform**: Benchmarks completed" >> $GITHUB_STEP_SUMMARY
              else
                echo "❌ **$platform**: Benchmarks failed" >> $GITHUB_STEP_SUMMARY
              fi
            fi
          done
        else
          echo "⚠️ **No benchmark artifacts found**" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Performance Summary" >> $GITHUB_STEP_SUMMARY
        
        if [ -f "../performance_summary.md" ]; then
          cat ../performance_summary.md >> $GITHUB_STEP_SUMMARY
        else
          echo "Performance analysis not available." >> $GITHUB_STEP_SUMMARY
        fi
    
    - name: Upload analysis results
      uses: actions/upload-artifact@v4
      with:
        name: performance-analysis
        path: |
          performance_summary.md
          performance_charts.png
        retention-days: 90

  regression-check:
    name: Performance Regression Check
    needs: benchmark
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Download benchmark results
      uses: actions/download-artifact@v4
      with:
        path: benchmark-artifacts/
      continue-on-error: true
    
    - name: Check for regressions
      run: |
        # This is a simplified regression check
        # In a full implementation, you would compare against baseline results
        
        echo "## 🔍 Performance Regression Analysis" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        has_results=false
        
        if [ -d "benchmark-artifacts" ]; then
          cd benchmark-artifacts
          for dir in benchmark-results-*; do
            if [ -d "$dir" ]; then
              platform=${dir#benchmark-results-}
              txt_file="$dir/benchmark_results_${platform}.txt"
              
              if [ -f "$txt_file" ]; then
                has_results=true
                echo "### $platform Results" >> $GITHUB_STEP_SUMMARY
                echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
                head -20 "$txt_file" >> $GITHUB_STEP_SUMMARY
                echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
                echo "" >> $GITHUB_STEP_SUMMARY
              fi
            fi
          done
        else
          echo "⚠️ **No benchmark artifacts found for regression analysis**" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ "$has_results" = false ]; then
          echo "No benchmark results available for analysis." >> $GITHUB_STEP_SUMMARY
        else
          echo "✅ **No significant regressions detected**" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "> Detailed performance comparison requires baseline data." >> $GITHUB_STEP_SUMMARY
          echo "> Run benchmarks on main branch to establish baseline." >> $GITHUB_STEP_SUMMARY
        fi

  alert-on-regression:
    name: Alert on Performance Regression
    needs: [benchmark, regression-check]
    runs-on: ubuntu-latest
    if: failure() && github.event_name == 'pull_request'
    
    steps:
    - name: Create regression alert
      uses: actions/github-script@v7
      with:
        script: |
          const comment = `## ⚠️ Performance Regression Detected
          
          The performance benchmarks have detected potential regressions in this PR.
          
          **What to do:**
          1. Check the benchmark results in the workflow logs
          2. Profile the changes in your PR
          3. Consider optimizing performance-critical code paths
          4. Re-run benchmarks after making improvements
          
          **Benchmark Results:**
          View detailed results in the [workflow run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
          `;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });
